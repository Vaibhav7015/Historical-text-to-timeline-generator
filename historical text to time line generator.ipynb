{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af7615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q langchain langchain-community langchain-huggingface sentence-transformers python-dotenv openai faiss-cpu PyMuPDF gradio tiktoken google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39467240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyAJVD0jTifmfrnXWnRd9_OGClZO_QzwKwA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Now you can use GOOGLE_API_KEY wherever required\n",
    "print(GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c31a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Globals\n",
    "faiss_db = None\n",
    "qa_chain = None\n",
    "all_docs = []\n",
    "year_event_map = {}  # new: year -> list of events\n",
    "\n",
    "\n",
    "def load_and_split_pdf(pdf_path):\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "def create_faiss_index(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "def create_qa_chain(faiss_index):\n",
    "    retriever = faiss_index.as_retriever(search_kwargs={\"k\": 5})\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.2,\n",
    "        google_api_key=GOOGLE_API_KEY  # make sure this is defined globally\n",
    "    )\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type=\"stuff\")\n",
    "\n",
    "\n",
    "def answer_question(question):\n",
    "    global qa_chain\n",
    "    if qa_chain is None:\n",
    "        return \"‚ùå Please upload and process a PDF first.\"\n",
    "    try:\n",
    "        result = qa_chain.run(question)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error answering question: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece51b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_timeline(): \n",
    "    global all_docs\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"‚ùå Please upload and process a PDF first.\"\n",
    "\n",
    "    from collections import defaultdict\n",
    "    import re\n",
    "\n",
    "    year_event_map = defaultdict(list)\n",
    "    pattern = r\"\\b(1[5-9]\\d{2}|20\\d{2})\\b\"  # Match years from 1500‚Äì2099\n",
    "\n",
    "    for doc in all_docs:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', doc.page_content)\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            # Skip short or noisy sentences\n",
    "            if len(sentence) < 30 or re.search(r'\\b(page|figure|table|chapter)\\b', sentence, re.IGNORECASE):\n",
    "                continue\n",
    "\n",
    "            matches = re.findall(pattern, sentence)\n",
    "            for year in matches:\n",
    "                year_event_map[int(year)].append(sentence)\n",
    "\n",
    "    if not year_event_map:\n",
    "        return \"‚ö†Ô∏è No historical events with years were found in the document.\"\n",
    "\n",
    "    # Format timeline with better structure\n",
    "    timeline = []\n",
    "    timeline.append(\"üìú Chronological Timeline of Events\")\n",
    "    timeline.append(\"=\" * 40)\n",
    "\n",
    "    for year in sorted(year_event_map.keys()):\n",
    "        timeline.append(f\"\\nüóì Year: {year}\")\n",
    "        timeline.append(\"-\" * 40)\n",
    "        events = list(dict.fromkeys(year_event_map[year]))[:2]  # Remove duplicates, max 2\n",
    "        for i, event in enumerate(events, start=1):\n",
    "            timeline.append(f\"{i}) {event}\")\n",
    "    \n",
    "    return \"\\n\".join(timeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0f05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def upload_pdf(file):\n",
    "    global faiss_db, qa_chain, all_docs\n",
    "\n",
    "    try:\n",
    "        if file is None:\n",
    "            return \"‚ùå No file uploaded.\"\n",
    "\n",
    "        file_path = file.name\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"‚ùå File not found: {file_path}\"\n",
    "\n",
    "        all_docs = load_and_split_pdf(file_path)\n",
    "        faiss_db = create_faiss_index(all_docs)\n",
    "        qa_chain = create_qa_chain(faiss_db)\n",
    "\n",
    "        return f\"‚úÖ Processed file: {os.path.basename(file_path)}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b90efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prart\\AppData\\Local\\Temp\\ipykernel_11188\\963941059.py:42: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain.run(question)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload History PDF\", file_types=[\".pdf\"], type=\"filepath\")\n",
    "        upload_btn = gr.Button(\"Process PDF\")\n",
    "    status = gr.Textbox(label=\"Status\")\n",
    "\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Ask a historical question (e.g., What happened in 1945?)\")\n",
    "        answer_btn = gr.Button(\"Get Answer\")\n",
    "    answer_output = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    with gr.Row():\n",
    "        timeline_btn = gr.Button(\"Generate Timeline\")\n",
    "        timeline_output = gr.Textbox(label=\"Chronological Timeline\", lines=20)\n",
    "\n",
    "    upload_btn.click(upload_pdf, inputs=[pdf_input], outputs=[status])\n",
    "    answer_btn.click(answer_question, inputs=[question_input], outputs=[answer_output])\n",
    "    timeline_btn.click(show_timeline, outputs=[timeline_output])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
